# ğŸ·Red-Wine-Quality-Prediction

This project predicts **wine quality** (good vs. bad) using machine learning algorithms on the **UCI Red Wine Quality dataset**.  
The goal is to compare multiple models and select the best performer based on accuracy.  

---

## ğŸ“‚ Dataset  

- **Source**: [Wine Quality Data Set - UCI ML Repo](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)  
- **File Used**: `winequality-red.csv`  
- **Shape**: `1599 rows Ã— 12 columns`  
- **Target Variable**:  
  - `quality` (scores 3â€“8)  
  - Converted into **binary classification**:  
    - `goodquality = 1` if quality â‰¥ 7  
    - `goodquality = 0` otherwise  

---

## ğŸ› ï¸ Workflow  

1. **Data Preprocessing**  
   - Checked for missing values â†’ none found  
   - Feature engineering: created `goodquality` label  
   - Exploratory Data Analysis (EDA)  
     - Boxplots, histograms, KDE plots  
     - Heatmap & correlation analysis  
     - Pairplots & violin plots  

2. **Feature Importance**  
   - Used `ExtraTreesClassifier` to find most important predictors.  
   - Top features: **alcohol, sulphates, volatile acidity**.  

3. **Model Training & Evaluation**  
   - Split data: 70% train, 30% test  
   - Models used:  
     - Logistic Regression  
     - K-Nearest Neighbors (KNN)  
     - Support Vector Classifier (SVC)  
     - Decision Tree  
     - Gaussian Naive Bayes  
     - Random Forest  
     - XGBoost  

---

## ğŸ“Š Results  

| Rank | Model                | Accuracy |
|------|----------------------|----------|
| ğŸ¥‡   | Random Forest        | **0.894** |
| ğŸ¥ˆ   | XGBoost              | 0.879    |
| ğŸ¥‰   | KNN                  | 0.872    |
| 4    | Logistic Regression  | 0.870    |
| 5    | SVC                  | 0.868    |
| 6    | Decision Tree        | 0.865    |
| 7    | GaussianNB           | 0.833    |

ğŸ‘‰ **Random Forest was selected as the final model** due to the highest accuracy.  

---

## ğŸ“¦ Libraries Used  

- **NumPy, Pandas, Matplotlib, Seaborn** â†’ data handling & visualization  
- **Scikit-learn** â†’ ML models (Logistic Regression, KNN, SVC, Decision Tree, RF, Naive Bayes)  
- **XGBoost** â†’ Gradient boosting classifier  

---

## ğŸ“Œ Future Improvements  

- **Hyperparameter tuning** â†’ GridSearchCV, RandomizedSearchCV  
- **Class imbalance handling** â†’ SMOTE, class weights  
- **Advanced models** â†’ LightGBM, CatBoost  
- **Cross-validation** â†’ More robust evaluation  
- **Ensemble stacking** â†’ Combine multiple models for better performance  
- **Deployment** â†’ Flask or Streamlit web interface  
- **Reproducibility** â†’ Dockerized version of the project  
